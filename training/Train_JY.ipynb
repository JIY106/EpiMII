{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eaef2d9-c797-4569-95d3-6e91ad33a15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import queue\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from utils_change1 import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
    "from model_utils import featurize, loss_smoothed, loss_nll, get_std_opt, ProteinMPNN\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc871ffd-6ee2-40dd-a180-60dc2c1f1c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    import json, time, os, sys, glob, os.path\n",
    "    import shutil\n",
    "    import warnings\n",
    "    import numpy as np  \n",
    "    import torch\n",
    "    from torch import optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    import queue\n",
    "    import copy\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import random\n",
    "    import os.path\n",
    "    import subprocess\n",
    "    import multiprocessing\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed    \n",
    "    from utils_change1 import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
    "    from model_utils import featurize, loss_smoothed, loss_nll, get_std_opt, ProteinMPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b199c1f-f1f0-47fb-aae2-fb03e538af4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\JIY106\\OneDrive - University of Pittsburgh\\Publications\\Redesign\\Github\\training\n"
     ]
    }
   ],
   "source": [
    "    cwd = os.getcwd()\n",
    "    print(\"Current working directory: {0}\".format(cwd))\n",
    "    #os.chdir('/ix1/zfeng/')\n",
    "    #print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774353ac-954f-4c05-b3f3-1132b1dfff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "<function is_available at 0x0000019317BC5C60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIY106\\AppData\\Local\\Temp\\ipykernel_15712\\3784145566.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"CPU\")\n",
    "    print(device)\n",
    "    print(torch.cuda.is_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae0e8d4-7d6d-4d5b-a112-bf013ccd97be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    path_for_outputs = \"Train_output\" #Input\n",
    "    base_folder = time.strftime(path_for_outputs, time.localtime())\n",
    "\n",
    "    if base_folder[-1] != '/':\n",
    "        base_folder += '/'\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    subfolders = ['model_weights']\n",
    "    for subfolder in subfolders:\n",
    "        if not os.path.exists(base_folder + subfolder):\n",
    "            os.makedirs(base_folder + subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f41921e-73e4-452e-a9cf-61d4e21537ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    PATH = [] #previous_checkpoint\n",
    "    previous_checkpoint = ''\n",
    "    logfile = base_folder + 'log1.txt'\n",
    "    if not PATH:\n",
    "        with open(logfile, 'w') as f:\n",
    "            f.write('Epoch\\tTrain\\tValidation\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6942f9-d1a4-4fed-b15b-0044be686723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    path_for_training_data = \"training_data\" #Input\n",
    "    data_path = path_for_training_data\n",
    "    params = {\n",
    "        \"LIST\"    : f\"{data_path}/list_142934.csv\", \n",
    "        \"VAL\"     : f\"{data_path}/validation.txt\",\n",
    "        \"TEST\"    : f\"{data_path}/test.txt\",\n",
    "        \"DIR\"     : f\"{data_path}\",\n",
    "        \"DATCUT\"  : \"2030-Jan-01\",\n",
    "        \"RESCUT\"  : 100000, \n",
    "        \"HOMO\"    : 0.70 \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5687dd80-3d95-4778-a9e9-12ff44359221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data\n",
      "{'LIST': 'training_data/list_142934.csv', 'VAL': 'training_data/validation.txt', 'TEST': 'training_data/test.txt', 'DIR': 'training_data', 'DATCUT': '2030-Jan-01', 'RESCUT': 100000, 'HOMO': 0.7}\n"
     ]
    }
   ],
   "source": [
    "    print(data_path)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96695b86-5044-4ce5-9340-da7f4efa52f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    LOAD_PARAM = {'batch_size': 1,\n",
    "                'shuffle': True,\n",
    "                'pin_memory':False,\n",
    "                'num_workers': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ca50ad-e4ae-4d6c-b8d7-96201623ae78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a580a4-be95-4403-ba9e-197ae3044fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    if debug is True:\n",
    "        num_examples_per_epoch = 50\n",
    "        max_protein_length = 1000\n",
    "        batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a171f7-d488-488e-bd2a-e5f266012ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    train, valid, test = build_training_clusters(params, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090bb182-0583-4524-a6bb-3848eae737be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14294 14293 114347\n"
     ]
    }
   ],
   "source": [
    "    print(len(test), len(valid), len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "263e6b29-9b53-40ec-b93a-1c2ae9442f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "    valid_set = PDB_dataset(list(valid.keys()), loader_pdb, valid, params)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "388eda8c-3971-4500-b26d-687314b0e654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    hidden_dim = 128 #Input, this is default\n",
    "    num_encoder_layers = 3 #Input, this is default\n",
    "    num_decoder_layers = 3 #Input, this is default\n",
    "    num_neighbors = 48 #Input\n",
    "    dropout = 0.1 #Input, this is default\n",
    "    backbone_noise = 0 #Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f58809-1ee0-4bd7-b3e5-eaec312d26ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinMPNN(\n",
       "  (features): ProteinFeatures(\n",
       "    (embeddings): PositionalEncodings(\n",
       "      (linear): Linear(in_features=66, out_features=16, bias=True)\n",
       "    )\n",
       "    (edge_embedding): Linear(in_features=416, out_features=128, bias=False)\n",
       "    (norm_edges): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (W_e): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (W_s): Embedding(21, 128)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-2): 3 x EncLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W11): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W13): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-2): 3 x DecLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (W_out): Linear(in_features=128, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    model = ProteinMPNN(node_features=hidden_dim, \n",
    "                        edge_features=hidden_dim, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        num_encoder_layers=num_encoder_layers, \n",
    "                        num_decoder_layers=num_encoder_layers, \n",
    "                        k_neighbors=num_neighbors, \n",
    "                        dropout=dropout, \n",
    "                        augment_eps=backbone_noise)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5765eb68-33a3-41dc-bf33-0ae9d98b2d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    if PATH:\n",
    "        checkpoint = torch.load(PATH)\n",
    "        total_step = checkpoint['step'] #write total_step from the checkpoint\n",
    "        epoch = checkpoint['epoch'] #write epoch from the checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        total_step = 0\n",
    "        epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3bee0a-d2fa-4e71-9e47-e24d26025d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    optimizer = get_std_opt(model.parameters(), hidden_dim, total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac5d4298-f856-4ad2-b9bf-19ed124307ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    if PATH:\n",
    "        optimizer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97922ae9-609f-4d54-bcc2-d298bbb7626f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    max_protein_length = 10000 #Input \n",
    "    num_examples_per_epoch = 1000000 #Input \n",
    "    batch_size = 2000 #Input\n",
    "    num_epochs = 200 #Input \n",
    "    reload_data_every_n_epochs = 4 #Input \n",
    "    gradient_norm = -1.0 #Input\n",
    "    save_model_every_n_epochs = 2 #Input\n",
    "    #debug = False\n",
    "    mixed_precision = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629050d6-1da9-440f-b3f0-b92dcbc0da22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.74it/s]\n"
     ]
    }
   ],
   "source": [
    "    with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "        q = queue.Queue(maxsize=3)\n",
    "        p = queue.Queue(maxsize=3)\n",
    "        for i in tqdm(range(3)):\n",
    "            q.put_nowait(executor.submit(get_pdbs, train_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "            p.put_nowait(executor.submit(get_pdbs, valid_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "\n",
    "        pdb_dict_train = q.get().result()\n",
    "        pdb_dict_valid = p.get().result() \n",
    "        #print(\"pdb_dict_train: \", pdb_dict_train) #add by myself\n",
    "        #print(\"pdb_dict_valid: \", pdb_dict_valid) #add by myself\n",
    "        dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=max_protein_length) \n",
    "        dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=max_protein_length)\n",
    "        #print(\"dataset_train: \", dataset_train) #add by myself\n",
    "        loader_train = StructureLoader(dataset_train, batch_size=batch_size)\n",
    "        print(\"Hey phd\")\n",
    "        loader_valid = StructureLoader(dataset_valid, batch_size=batch_size)\n",
    "        #print(\"loader_train: \", list(loader_train)) #add by myself\n",
    "        reload_c = 0 \n",
    "        for e in tqdm(range(num_epochs)):\n",
    "            t0 = time.time()\n",
    "            e = epoch + e\n",
    "            model.train()\n",
    "            train_sum, train_weights = 0., 0.\n",
    "            train_acc = 0.\n",
    "            #print(\"train_weights: \", train_weights) #add by myself\n",
    "            if e % reload_data_every_n_epochs == 0:\n",
    "                if reload_c != 0:\n",
    "                    pdb_dict_train = q.get().result()\n",
    "                    dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=max_protein_length)\n",
    "                    loader_train = StructureLoader(dataset_train, batch_size=batch_size)\n",
    "                    pdb_dict_valid = p.get().result()\n",
    "                    dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=max_protein_length)\n",
    "                    loader_valid = StructureLoader(dataset_valid, batch_size=batch_size)\n",
    "                    q.put_nowait(executor.submit(get_pdbs, train_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "                    p.put_nowait(executor.submit(get_pdbs, valid_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "                reload_c += 1\n",
    "            #print(\"reload_c: \", reload_c) #add by myself\n",
    "            for _, batch in enumerate(loader_train):\n",
    "                start_batch = time.time()\n",
    "                X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "                elapsed_featurize = time.time() - start_batch\n",
    "                optimizer.zero_grad()\n",
    "                mask_for_loss = mask*chain_M\n",
    "                \n",
    "                #print(\"Mask:\", mask) #add by myself\n",
    "                #print(\"Chain_M:\", chain_M) #add by myself\n",
    "                #print(\"Mask_for_loss:\", mask_for_loss) #add by myself\n",
    "                \n",
    "                if mixed_precision is True: #Input, this is default\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                        _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "           \n",
    "                    scaler.scale(loss_av_smoothed).backward()\n",
    "                     \n",
    "                    if gradient_norm > 0.0:\n",
    "                        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                    _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "                    loss_av_smoothed.backward()\n",
    "\n",
    "                    if gradient_norm > 0.0:\n",
    "                        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "                loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "            \n",
    "                train_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "                train_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "                \n",
    "                #debugging\n",
    "                #print(\"Before addition: train_weights =\", train_weights) #add by myself\n",
    "                #print(\"mask_for_loss =\", mask_for_loss) #add by myself\n",
    "                \n",
    "                train_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "                \n",
    "                #debugging\n",
    "                #print(\"After addition: train_weights =\", train_weights) #add by myself\n",
    "                \n",
    "                total_step += 1\n",
    "                #print(\"Total_step:\", total_step) #add by myself\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validation_sum, validation_weights = 0., 0.\n",
    "                validation_acc = 0.\n",
    "                for _, batch in enumerate(loader_valid):\n",
    "                    X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "                    log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                    mask_for_loss = mask*chain_M\n",
    "                    loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "                    \n",
    "                    validation_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "                    validation_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "                    validation_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "            \n",
    "            #if train_weights == 0: #add by myself\n",
    "                #print(\"Cannot be divided by zero\") #add by myself\n",
    "            #else: #add by myself\n",
    "                #print(\"train_weights is not zero\")\n",
    "            train_loss = train_sum / train_weights\n",
    "            train_accuracy = train_acc / train_weights\n",
    "            train_perplexity = np.exp(train_loss)\n",
    "            validation_loss = validation_sum / validation_weights\n",
    "            validation_accuracy = validation_acc / validation_weights\n",
    "            validation_perplexity = np.exp(validation_loss)\n",
    "            \n",
    "            train_perplexity_ = np.format_float_positional(np.float32(train_perplexity), unique=False, precision=3)     \n",
    "            validation_perplexity_ = np.format_float_positional(np.float32(validation_perplexity), unique=False, precision=3)\n",
    "            train_accuracy_ = np.format_float_positional(np.float32(train_accuracy), unique=False, precision=3)\n",
    "            validation_accuracy_ = np.format_float_positional(np.float32(validation_accuracy), unique=False, precision=3)\n",
    "    \n",
    "            t1 = time.time()\n",
    "            dt = np.format_float_positional(np.float32(t1-t0), unique=False, precision=1) \n",
    "            with open(logfile, 'a') as f:\n",
    "                f.write(f'epoch: {e+1}, step: {total_step}, time: {dt}, train: {train_perplexity_}, valid: {validation_perplexity_}, train_acc: {train_accuracy_}, valid_acc: {validation_accuracy_}\\n')\n",
    "            print(f'epoch: {e+1}, step: {total_step}, time: {dt}, train: {train_perplexity_}, valid: {validation_perplexity_}, train_acc: {train_accuracy_}, valid_acc: {validation_accuracy_}')\n",
    "            \n",
    "            checkpoint_filename_last = base_folder+'model_weights/epoch_last.pt'.format(e+1, total_step)\n",
    "            torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'step': total_step,\n",
    "                        'num_edges' : num_neighbors,\n",
    "                        'noise_level': backbone_noise,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.optimizer.state_dict(),\n",
    "                        }, checkpoint_filename_last)\n",
    "\n",
    "            if (e+1) % save_model_every_n_epochs == 0:\n",
    "                checkpoint_filename = base_folder+'model_weights/epoch{}_step{}.pt'.format(e+1, total_step)\n",
    "                torch.save({\n",
    "                        'epoch': e+1,\n",
    "                        'step': total_step,\n",
    "                        'num_edges' : num_neighbors,\n",
    "                        'noise_level': backbone_noise, \n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.optimizer.state_dict(),\n",
    "                        }, checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16b82a-e1de-4711-ac62-9c2c4f9474b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
